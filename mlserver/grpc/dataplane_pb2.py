# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: dataplane.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x0f\x64\x61taplane.proto\x12\x12inference_mlserver"\x13\n\x11ServerLiveRequest""\n\x12ServerLiveResponse\x12\x0c\n\x04live\x18\x01 \x01(\x08"\x14\n\x12ServerReadyRequest"$\n\x13ServerReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08"2\n\x11ModelReadyRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t"#\n\x12ModelReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08"\x17\n\x15ServerMetadataRequest"K\n\x16ServerMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\x12\x12\n\nextensions\x18\x03 \x03(\t"5\n\x14ModelMetadataRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t"\xfb\x04\n\x15ModelMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08versions\x18\x02 \x03(\t\x12\x10\n\x08platform\x18\x03 \x01(\t\x12H\n\x06inputs\x18\x04 \x03(\x0b\x32\x38.inference_mlserver.ModelMetadataResponse.TensorMetadata\x12I\n\x07outputs\x18\x05 \x03(\x0b\x32\x38.inference_mlserver.ModelMetadataResponse.TensorMetadata\x12M\n\nparameters\x18\x06 \x03(\x0b\x32\x39.inference_mlserver.ModelMetadataResponse.ParametersEntry\x1a\xf4\x01\n\x0eTensorMetadata\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12\\\n\nparameters\x18\x04 \x03(\x0b\x32H.inference_mlserver.ModelMetadataResponse.TensorMetadata.ParametersEntry\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01"\xbf\x07\n\x11ModelInferRequest\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\t\x12\n\n\x02id\x18\x03 \x01(\t\x12I\n\nparameters\x18\x04 \x03(\x0b\x32\x35.inference_mlserver.ModelInferRequest.ParametersEntry\x12\x46\n\x06inputs\x18\x05 \x03(\x0b\x32\x36.inference_mlserver.ModelInferRequest.InferInputTensor\x12Q\n\x07outputs\x18\x06 \x03(\x0b\x32@.inference_mlserver.ModelInferRequest.InferRequestedOutputTensor\x12\x1a\n\x12raw_input_contents\x18\x07 \x03(\x0c\x1a\xaf\x02\n\x10InferInputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12Z\n\nparameters\x18\x04 \x03(\x0b\x32\x46.inference_mlserver.ModelInferRequest.InferInputTensor.ParametersEntry\x12\x39\n\x08\x63ontents\x18\x05 \x01(\x0b\x32\'.inference_mlserver.InferTensorContents\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01\x1a\xe7\x01\n\x1aInferRequestedOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x64\n\nparameters\x18\x02 \x03(\x0b\x32P.inference_mlserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01"\x8b\x05\n\x12ModelInferResponse\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\t\x12\n\n\x02id\x18\x03 \x01(\t\x12J\n\nparameters\x18\x04 \x03(\x0b\x32\x36.inference_mlserver.ModelInferResponse.ParametersEntry\x12I\n\x07outputs\x18\x05 \x03(\x0b\x32\x38.inference_mlserver.ModelInferResponse.InferOutputTensor\x12\x1b\n\x13raw_output_contents\x18\x06 \x03(\x0c\x1a\xb2\x02\n\x11InferOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12\\\n\nparameters\x18\x04 \x03(\x0b\x32H.inference_mlserver.ModelInferResponse.InferOutputTensor.ParametersEntry\x12\x39\n\x08\x63ontents\x18\x05 \x01(\x0b\x32\'.inference_mlserver.InferTensorContents\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01\x1aU\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x31\n\x05value\x18\x02 \x01(\x0b\x32".inference_mlserver.InferParameter:\x02\x38\x01"i\n\x0eInferParameter\x12\x14\n\nbool_param\x18\x01 \x01(\x08H\x00\x12\x15\n\x0bint64_param\x18\x02 \x01(\x03H\x00\x12\x16\n\x0cstring_param\x18\x03 \x01(\tH\x00\x42\x12\n\x10parameter_choice"\xd0\x01\n\x13InferTensorContents\x12\x15\n\rbool_contents\x18\x01 \x03(\x08\x12\x14\n\x0cint_contents\x18\x02 \x03(\x05\x12\x16\n\x0eint64_contents\x18\x03 \x03(\x03\x12\x15\n\ruint_contents\x18\x04 \x03(\r\x12\x17\n\x0fuint64_contents\x18\x05 \x03(\x04\x12\x15\n\rfp32_contents\x18\x06 \x03(\x02\x12\x15\n\rfp64_contents\x18\x07 \x03(\x01\x12\x16\n\x0e\x62ytes_contents\x18\x08 \x03(\x0c"\x8a\x01\n\x18ModelRepositoryParameter\x12\x14\n\nbool_param\x18\x01 \x01(\x08H\x00\x12\x15\n\x0bint64_param\x18\x02 \x01(\x03H\x00\x12\x16\n\x0cstring_param\x18\x03 \x01(\tH\x00\x12\x15\n\x0b\x62ytes_param\x18\x04 \x01(\x0cH\x00\x42\x12\n\x10parameter_choice"@\n\x16RepositoryIndexRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\x12\r\n\x05ready\x18\x02 \x01(\x08"\xad\x01\n\x17RepositoryIndexResponse\x12\x46\n\x06models\x18\x01 \x03(\x0b\x32\x36.inference_mlserver.RepositoryIndexResponse.ModelIndex\x1aJ\n\nModelIndex\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\x12\r\n\x05state\x18\x03 \x01(\t\x12\x0e\n\x06reason\x18\x04 \x01(\t"\xfe\x01\n\x1aRepositoryModelLoadRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\x12R\n\nparameters\x18\x03 \x03(\x0b\x32>.inference_mlserver.RepositoryModelLoadRequest.ParametersEntry\x1a_\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.inference_mlserver.ModelRepositoryParameter:\x02\x38\x01"\x1d\n\x1bRepositoryModelLoadResponse"\x82\x02\n\x1cRepositoryModelUnloadRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\x12T\n\nparameters\x18\x03 \x03(\x0b\x32@.inference_mlserver.RepositoryModelUnloadRequest.ParametersEntry\x1a_\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.inference_mlserver.ModelRepositoryParameter:\x02\x38\x01"\x1f\n\x1dRepositoryModelUnloadResponse2\xd0\x07\n\x14GRPCInferenceService\x12]\n\nServerLive\x12%.inference_mlserver.ServerLiveRequest\x1a&.inference_mlserver.ServerLiveResponse"\x00\x12`\n\x0bServerReady\x12&.inference_mlserver.ServerReadyRequest\x1a\'.inference_mlserver.ServerReadyResponse"\x00\x12]\n\nModelReady\x12%.inference_mlserver.ModelReadyRequest\x1a&.inference_mlserver.ModelReadyResponse"\x00\x12i\n\x0eServerMetadata\x12).inference_mlserver.ServerMetadataRequest\x1a*.inference_mlserver.ServerMetadataResponse"\x00\x12\x66\n\rModelMetadata\x12(.inference_mlserver.ModelMetadataRequest\x1a).inference_mlserver.ModelMetadataResponse"\x00\x12]\n\nModelInfer\x12%.inference_mlserver.ModelInferRequest\x1a&.inference_mlserver.ModelInferResponse"\x00\x12l\n\x0fRepositoryIndex\x12*.inference_mlserver.RepositoryIndexRequest\x1a+.inference_mlserver.RepositoryIndexResponse"\x00\x12x\n\x13RepositoryModelLoad\x12..inference_mlserver.RepositoryModelLoadRequest\x1a/.inference_mlserver.RepositoryModelLoadResponse"\x00\x12~\n\x15RepositoryModelUnload\x12\x30.inference_mlserver.RepositoryModelUnloadRequest\x1a\x31.inference_mlserver.RepositoryModelUnloadResponse"\x00\x62\x06proto3'
)


_SERVERLIVEREQUEST = DESCRIPTOR.message_types_by_name["ServerLiveRequest"]
_SERVERLIVERESPONSE = DESCRIPTOR.message_types_by_name["ServerLiveResponse"]
_SERVERREADYREQUEST = DESCRIPTOR.message_types_by_name["ServerReadyRequest"]
_SERVERREADYRESPONSE = DESCRIPTOR.message_types_by_name["ServerReadyResponse"]
_MODELREADYREQUEST = DESCRIPTOR.message_types_by_name["ModelReadyRequest"]
_MODELREADYRESPONSE = DESCRIPTOR.message_types_by_name["ModelReadyResponse"]
_SERVERMETADATAREQUEST = DESCRIPTOR.message_types_by_name["ServerMetadataRequest"]
_SERVERMETADATARESPONSE = DESCRIPTOR.message_types_by_name["ServerMetadataResponse"]
_MODELMETADATAREQUEST = DESCRIPTOR.message_types_by_name["ModelMetadataRequest"]
_MODELMETADATARESPONSE = DESCRIPTOR.message_types_by_name["ModelMetadataResponse"]
_MODELMETADATARESPONSE_TENSORMETADATA = _MODELMETADATARESPONSE.nested_types_by_name[
    "TensorMetadata"
]
_MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY = (
    _MODELMETADATARESPONSE_TENSORMETADATA.nested_types_by_name["ParametersEntry"]
)
_MODELMETADATARESPONSE_PARAMETERSENTRY = _MODELMETADATARESPONSE.nested_types_by_name[
    "ParametersEntry"
]
_MODELINFERREQUEST = DESCRIPTOR.message_types_by_name["ModelInferRequest"]
_MODELINFERREQUEST_INFERINPUTTENSOR = _MODELINFERREQUEST.nested_types_by_name[
    "InferInputTensor"
]
_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY = (
    _MODELINFERREQUEST_INFERINPUTTENSOR.nested_types_by_name["ParametersEntry"]
)
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR = _MODELINFERREQUEST.nested_types_by_name[
    "InferRequestedOutputTensor"
]
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY = (
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR.nested_types_by_name[
        "ParametersEntry"
    ]
)
_MODELINFERREQUEST_PARAMETERSENTRY = _MODELINFERREQUEST.nested_types_by_name[
    "ParametersEntry"
]
_MODELINFERRESPONSE = DESCRIPTOR.message_types_by_name["ModelInferResponse"]
_MODELINFERRESPONSE_INFEROUTPUTTENSOR = _MODELINFERRESPONSE.nested_types_by_name[
    "InferOutputTensor"
]
_MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY = (
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR.nested_types_by_name["ParametersEntry"]
)
_MODELINFERRESPONSE_PARAMETERSENTRY = _MODELINFERRESPONSE.nested_types_by_name[
    "ParametersEntry"
]
_INFERPARAMETER = DESCRIPTOR.message_types_by_name["InferParameter"]
_INFERTENSORCONTENTS = DESCRIPTOR.message_types_by_name["InferTensorContents"]
_MODELREPOSITORYPARAMETER = DESCRIPTOR.message_types_by_name["ModelRepositoryParameter"]
_REPOSITORYINDEXREQUEST = DESCRIPTOR.message_types_by_name["RepositoryIndexRequest"]
_REPOSITORYINDEXRESPONSE = DESCRIPTOR.message_types_by_name["RepositoryIndexResponse"]
_REPOSITORYINDEXRESPONSE_MODELINDEX = _REPOSITORYINDEXRESPONSE.nested_types_by_name[
    "ModelIndex"
]
_REPOSITORYMODELLOADREQUEST = DESCRIPTOR.message_types_by_name[
    "RepositoryModelLoadRequest"
]
_REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY = (
    _REPOSITORYMODELLOADREQUEST.nested_types_by_name["ParametersEntry"]
)
_REPOSITORYMODELLOADRESPONSE = DESCRIPTOR.message_types_by_name[
    "RepositoryModelLoadResponse"
]
_REPOSITORYMODELUNLOADREQUEST = DESCRIPTOR.message_types_by_name[
    "RepositoryModelUnloadRequest"
]
_REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY = (
    _REPOSITORYMODELUNLOADREQUEST.nested_types_by_name["ParametersEntry"]
)
_REPOSITORYMODELUNLOADRESPONSE = DESCRIPTOR.message_types_by_name[
    "RepositoryModelUnloadResponse"
]
ServerLiveRequest = _reflection.GeneratedProtocolMessageType(
    "ServerLiveRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERLIVEREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerLiveRequest)
    },
)
_sym_db.RegisterMessage(ServerLiveRequest)

ServerLiveResponse = _reflection.GeneratedProtocolMessageType(
    "ServerLiveResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERLIVERESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerLiveResponse)
    },
)
_sym_db.RegisterMessage(ServerLiveResponse)

ServerReadyRequest = _reflection.GeneratedProtocolMessageType(
    "ServerReadyRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERREADYREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerReadyRequest)
    },
)
_sym_db.RegisterMessage(ServerReadyRequest)

ServerReadyResponse = _reflection.GeneratedProtocolMessageType(
    "ServerReadyResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERREADYRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerReadyResponse)
    },
)
_sym_db.RegisterMessage(ServerReadyResponse)

ModelReadyRequest = _reflection.GeneratedProtocolMessageType(
    "ModelReadyRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _MODELREADYREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelReadyRequest)
    },
)
_sym_db.RegisterMessage(ModelReadyRequest)

ModelReadyResponse = _reflection.GeneratedProtocolMessageType(
    "ModelReadyResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _MODELREADYRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelReadyResponse)
    },
)
_sym_db.RegisterMessage(ModelReadyResponse)

ServerMetadataRequest = _reflection.GeneratedProtocolMessageType(
    "ServerMetadataRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERMETADATAREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerMetadataRequest)
    },
)
_sym_db.RegisterMessage(ServerMetadataRequest)

ServerMetadataResponse = _reflection.GeneratedProtocolMessageType(
    "ServerMetadataResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _SERVERMETADATARESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ServerMetadataResponse)
    },
)
_sym_db.RegisterMessage(ServerMetadataResponse)

ModelMetadataRequest = _reflection.GeneratedProtocolMessageType(
    "ModelMetadataRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _MODELMETADATAREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelMetadataRequest)
    },
)
_sym_db.RegisterMessage(ModelMetadataRequest)

ModelMetadataResponse = _reflection.GeneratedProtocolMessageType(
    "ModelMetadataResponse",
    (_message.Message,),
    {
        "TensorMetadata": _reflection.GeneratedProtocolMessageType(
            "TensorMetadata",
            (_message.Message,),
            {
                "ParametersEntry": _reflection.GeneratedProtocolMessageType(
                    "ParametersEntry",
                    (_message.Message,),
                    {
                        "DESCRIPTOR": _MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY,
                        "__module__": "dataplane_pb2",
                        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelMetadataResponse.TensorMetadata.ParametersEntry)
                    },
                ),
                "DESCRIPTOR": _MODELMETADATARESPONSE_TENSORMETADATA,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelMetadataResponse.TensorMetadata)
            },
        ),
        "ParametersEntry": _reflection.GeneratedProtocolMessageType(
            "ParametersEntry",
            (_message.Message,),
            {
                "DESCRIPTOR": _MODELMETADATARESPONSE_PARAMETERSENTRY,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelMetadataResponse.ParametersEntry)
            },
        ),
        "DESCRIPTOR": _MODELMETADATARESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelMetadataResponse)
    },
)
_sym_db.RegisterMessage(ModelMetadataResponse)
_sym_db.RegisterMessage(ModelMetadataResponse.TensorMetadata)
_sym_db.RegisterMessage(ModelMetadataResponse.TensorMetadata.ParametersEntry)
_sym_db.RegisterMessage(ModelMetadataResponse.ParametersEntry)

ModelInferRequest = _reflection.GeneratedProtocolMessageType(
    "ModelInferRequest",
    (_message.Message,),
    {
        "InferInputTensor": _reflection.GeneratedProtocolMessageType(
            "InferInputTensor",
            (_message.Message,),
            {
                "ParametersEntry": _reflection.GeneratedProtocolMessageType(
                    "ParametersEntry",
                    (_message.Message,),
                    {
                        "DESCRIPTOR": _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY,
                        "__module__": "dataplane_pb2",
                        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest.InferInputTensor.ParametersEntry)
                    },
                ),
                "DESCRIPTOR": _MODELINFERREQUEST_INFERINPUTTENSOR,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest.InferInputTensor)
            },
        ),
        "InferRequestedOutputTensor": _reflection.GeneratedProtocolMessageType(
            "InferRequestedOutputTensor",
            (_message.Message,),
            {
                "ParametersEntry": _reflection.GeneratedProtocolMessageType(
                    "ParametersEntry",
                    (_message.Message,),
                    {
                        "DESCRIPTOR": _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY,
                        "__module__": "dataplane_pb2",
                        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
                    },
                ),
                "DESCRIPTOR": _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest.InferRequestedOutputTensor)
            },
        ),
        "ParametersEntry": _reflection.GeneratedProtocolMessageType(
            "ParametersEntry",
            (_message.Message,),
            {
                "DESCRIPTOR": _MODELINFERREQUEST_PARAMETERSENTRY,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest.ParametersEntry)
            },
        ),
        "DESCRIPTOR": _MODELINFERREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferRequest)
    },
)
_sym_db.RegisterMessage(ModelInferRequest)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.ParametersEntry)

ModelInferResponse = _reflection.GeneratedProtocolMessageType(
    "ModelInferResponse",
    (_message.Message,),
    {
        "InferOutputTensor": _reflection.GeneratedProtocolMessageType(
            "InferOutputTensor",
            (_message.Message,),
            {
                "ParametersEntry": _reflection.GeneratedProtocolMessageType(
                    "ParametersEntry",
                    (_message.Message,),
                    {
                        "DESCRIPTOR": _MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY,
                        "__module__": "dataplane_pb2",
                        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferResponse.InferOutputTensor.ParametersEntry)
                    },
                ),
                "DESCRIPTOR": _MODELINFERRESPONSE_INFEROUTPUTTENSOR,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferResponse.InferOutputTensor)
            },
        ),
        "ParametersEntry": _reflection.GeneratedProtocolMessageType(
            "ParametersEntry",
            (_message.Message,),
            {
                "DESCRIPTOR": _MODELINFERRESPONSE_PARAMETERSENTRY,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferResponse.ParametersEntry)
            },
        ),
        "DESCRIPTOR": _MODELINFERRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelInferResponse)
    },
)
_sym_db.RegisterMessage(ModelInferResponse)
_sym_db.RegisterMessage(ModelInferResponse.InferOutputTensor)
_sym_db.RegisterMessage(ModelInferResponse.InferOutputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferResponse.ParametersEntry)

InferParameter = _reflection.GeneratedProtocolMessageType(
    "InferParameter",
    (_message.Message,),
    {
        "DESCRIPTOR": _INFERPARAMETER,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.InferParameter)
    },
)
_sym_db.RegisterMessage(InferParameter)

InferTensorContents = _reflection.GeneratedProtocolMessageType(
    "InferTensorContents",
    (_message.Message,),
    {
        "DESCRIPTOR": _INFERTENSORCONTENTS,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.InferTensorContents)
    },
)
_sym_db.RegisterMessage(InferTensorContents)

ModelRepositoryParameter = _reflection.GeneratedProtocolMessageType(
    "ModelRepositoryParameter",
    (_message.Message,),
    {
        "DESCRIPTOR": _MODELREPOSITORYPARAMETER,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.ModelRepositoryParameter)
    },
)
_sym_db.RegisterMessage(ModelRepositoryParameter)

RepositoryIndexRequest = _reflection.GeneratedProtocolMessageType(
    "RepositoryIndexRequest",
    (_message.Message,),
    {
        "DESCRIPTOR": _REPOSITORYINDEXREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryIndexRequest)
    },
)
_sym_db.RegisterMessage(RepositoryIndexRequest)

RepositoryIndexResponse = _reflection.GeneratedProtocolMessageType(
    "RepositoryIndexResponse",
    (_message.Message,),
    {
        "ModelIndex": _reflection.GeneratedProtocolMessageType(
            "ModelIndex",
            (_message.Message,),
            {
                "DESCRIPTOR": _REPOSITORYINDEXRESPONSE_MODELINDEX,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryIndexResponse.ModelIndex)
            },
        ),
        "DESCRIPTOR": _REPOSITORYINDEXRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryIndexResponse)
    },
)
_sym_db.RegisterMessage(RepositoryIndexResponse)
_sym_db.RegisterMessage(RepositoryIndexResponse.ModelIndex)

RepositoryModelLoadRequest = _reflection.GeneratedProtocolMessageType(
    "RepositoryModelLoadRequest",
    (_message.Message,),
    {
        "ParametersEntry": _reflection.GeneratedProtocolMessageType(
            "ParametersEntry",
            (_message.Message,),
            {
                "DESCRIPTOR": _REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelLoadRequest.ParametersEntry)
            },
        ),
        "DESCRIPTOR": _REPOSITORYMODELLOADREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelLoadRequest)
    },
)
_sym_db.RegisterMessage(RepositoryModelLoadRequest)
_sym_db.RegisterMessage(RepositoryModelLoadRequest.ParametersEntry)

RepositoryModelLoadResponse = _reflection.GeneratedProtocolMessageType(
    "RepositoryModelLoadResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _REPOSITORYMODELLOADRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelLoadResponse)
    },
)
_sym_db.RegisterMessage(RepositoryModelLoadResponse)

RepositoryModelUnloadRequest = _reflection.GeneratedProtocolMessageType(
    "RepositoryModelUnloadRequest",
    (_message.Message,),
    {
        "ParametersEntry": _reflection.GeneratedProtocolMessageType(
            "ParametersEntry",
            (_message.Message,),
            {
                "DESCRIPTOR": _REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY,
                "__module__": "dataplane_pb2",
                # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelUnloadRequest.ParametersEntry)
            },
        ),
        "DESCRIPTOR": _REPOSITORYMODELUNLOADREQUEST,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelUnloadRequest)
    },
)
_sym_db.RegisterMessage(RepositoryModelUnloadRequest)
_sym_db.RegisterMessage(RepositoryModelUnloadRequest.ParametersEntry)

RepositoryModelUnloadResponse = _reflection.GeneratedProtocolMessageType(
    "RepositoryModelUnloadResponse",
    (_message.Message,),
    {
        "DESCRIPTOR": _REPOSITORYMODELUNLOADRESPONSE,
        "__module__": "dataplane_pb2",
        # @@protoc_insertion_point(class_scope:inference_mlserver.RepositoryModelUnloadResponse)
    },
)
_sym_db.RegisterMessage(RepositoryModelUnloadResponse)

_GRPCINFERENCESERVICE = DESCRIPTOR.services_by_name["GRPCInferenceService"]
if _descriptor._USE_C_DESCRIPTORS == False:

    DESCRIPTOR._options = None
    _MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY._options = None
    _MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY._serialized_options = b"8\001"
    _MODELMETADATARESPONSE_PARAMETERSENTRY._options = None
    _MODELMETADATARESPONSE_PARAMETERSENTRY._serialized_options = b"8\001"
    _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._options = None
    _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._serialized_options = b"8\001"
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._options = None
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._serialized_options = (
        b"8\001"
    )
    _MODELINFERREQUEST_PARAMETERSENTRY._options = None
    _MODELINFERREQUEST_PARAMETERSENTRY._serialized_options = b"8\001"
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY._options = None
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY._serialized_options = b"8\001"
    _MODELINFERRESPONSE_PARAMETERSENTRY._options = None
    _MODELINFERRESPONSE_PARAMETERSENTRY._serialized_options = b"8\001"
    _REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY._options = None
    _REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY._serialized_options = b"8\001"
    _REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY._options = None
    _REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY._serialized_options = b"8\001"
    _SERVERLIVEREQUEST._serialized_start = 39
    _SERVERLIVEREQUEST._serialized_end = 58
    _SERVERLIVERESPONSE._serialized_start = 60
    _SERVERLIVERESPONSE._serialized_end = 94
    _SERVERREADYREQUEST._serialized_start = 96
    _SERVERREADYREQUEST._serialized_end = 116
    _SERVERREADYRESPONSE._serialized_start = 118
    _SERVERREADYRESPONSE._serialized_end = 154
    _MODELREADYREQUEST._serialized_start = 156
    _MODELREADYREQUEST._serialized_end = 206
    _MODELREADYRESPONSE._serialized_start = 208
    _MODELREADYRESPONSE._serialized_end = 243
    _SERVERMETADATAREQUEST._serialized_start = 245
    _SERVERMETADATAREQUEST._serialized_end = 268
    _SERVERMETADATARESPONSE._serialized_start = 270
    _SERVERMETADATARESPONSE._serialized_end = 345
    _MODELMETADATAREQUEST._serialized_start = 347
    _MODELMETADATAREQUEST._serialized_end = 400
    _MODELMETADATARESPONSE._serialized_start = 403
    _MODELMETADATARESPONSE._serialized_end = 1038
    _MODELMETADATARESPONSE_TENSORMETADATA._serialized_start = 707
    _MODELMETADATARESPONSE_TENSORMETADATA._serialized_end = 951
    _MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY._serialized_start = 866
    _MODELMETADATARESPONSE_TENSORMETADATA_PARAMETERSENTRY._serialized_end = 951
    _MODELMETADATARESPONSE_PARAMETERSENTRY._serialized_start = 866
    _MODELMETADATARESPONSE_PARAMETERSENTRY._serialized_end = 951
    _MODELINFERREQUEST._serialized_start = 1041
    _MODELINFERREQUEST._serialized_end = 2000
    _MODELINFERREQUEST_INFERINPUTTENSOR._serialized_start = 1376
    _MODELINFERREQUEST_INFERINPUTTENSOR._serialized_end = 1679
    _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._serialized_start = 866
    _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._serialized_end = 951
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR._serialized_start = 1682
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR._serialized_end = 1913
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._serialized_start = (
        866
    )
    _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._serialized_end = 951
    _MODELINFERREQUEST_PARAMETERSENTRY._serialized_start = 866
    _MODELINFERREQUEST_PARAMETERSENTRY._serialized_end = 951
    _MODELINFERRESPONSE._serialized_start = 2003
    _MODELINFERRESPONSE._serialized_end = 2654
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR._serialized_start = 2261
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR._serialized_end = 2567
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY._serialized_start = 866
    _MODELINFERRESPONSE_INFEROUTPUTTENSOR_PARAMETERSENTRY._serialized_end = 951
    _MODELINFERRESPONSE_PARAMETERSENTRY._serialized_start = 866
    _MODELINFERRESPONSE_PARAMETERSENTRY._serialized_end = 951
    _INFERPARAMETER._serialized_start = 2656
    _INFERPARAMETER._serialized_end = 2761
    _INFERTENSORCONTENTS._serialized_start = 2764
    _INFERTENSORCONTENTS._serialized_end = 2972
    _MODELREPOSITORYPARAMETER._serialized_start = 2975
    _MODELREPOSITORYPARAMETER._serialized_end = 3113
    _REPOSITORYINDEXREQUEST._serialized_start = 3115
    _REPOSITORYINDEXREQUEST._serialized_end = 3179
    _REPOSITORYINDEXRESPONSE._serialized_start = 3182
    _REPOSITORYINDEXRESPONSE._serialized_end = 3355
    _REPOSITORYINDEXRESPONSE_MODELINDEX._serialized_start = 3281
    _REPOSITORYINDEXRESPONSE_MODELINDEX._serialized_end = 3355
    _REPOSITORYMODELLOADREQUEST._serialized_start = 3358
    _REPOSITORYMODELLOADREQUEST._serialized_end = 3612
    _REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY._serialized_start = 3517
    _REPOSITORYMODELLOADREQUEST_PARAMETERSENTRY._serialized_end = 3612
    _REPOSITORYMODELLOADRESPONSE._serialized_start = 3614
    _REPOSITORYMODELLOADRESPONSE._serialized_end = 3643
    _REPOSITORYMODELUNLOADREQUEST._serialized_start = 3646
    _REPOSITORYMODELUNLOADREQUEST._serialized_end = 3904
    _REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY._serialized_start = 3517
    _REPOSITORYMODELUNLOADREQUEST_PARAMETERSENTRY._serialized_end = 3612
    _REPOSITORYMODELUNLOADRESPONSE._serialized_start = 3906
    _REPOSITORYMODELUNLOADRESPONSE._serialized_end = 3937
    _GRPCINFERENCESERVICE._serialized_start = 3940
    _GRPCINFERENCESERVICE._serialized_end = 4916
# @@protoc_insertion_point(module_scope)
